{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f86f3fb-01ca-4678-b495-8407860d2178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\alex-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     C:\\Users\\alex-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package shakespeare is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\alex-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alex-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import shakespeare\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')\n",
    "nltk.download('shakespeare')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "25ae9871-0ebb-4d56-b16c-77c554775894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\alex-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alex-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "52740cdf-b661-419e-9a58-259b4c9d2a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\alex-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load movie reviews for classification\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "reviews = [(movie_reviews.words(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Preprocess the dataset\n",
    "\n",
    "X = [' '.join(words) for words, _ in reviews]\n",
    "y = [category for _, category in reviews]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c5cd6418-f083-46c1-8b7e-f3f332d570a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "def tokenize(corpus):\n",
    "    return [nltk.word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "# Train Word2Vec models\n",
    "def train_word2vec(corpus, vector_size=100):\n",
    "    tokenized_corpus = tokenize(corpus)\n",
    "    model = gensim.models.Word2Vec(sentences=tokenized_corpus, vector_size=vector_size, window=5, sg=1, min_count=2)\n",
    "    return model\n",
    "\n",
    "# Represent movie reviews using averaged word embeddings\n",
    "def get_document_vector(doc, model):\n",
    "    words = nltk.word_tokenize(doc.lower())\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1f7b67d4-7600-4f08-9084-2e670c42c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240577\n"
     ]
    }
   ],
   "source": [
    "# Train model on Shakespeare\n",
    "\n",
    "shakespeare_corpus = [' '.join(shakespeare.words(fileid)) for fileid in shakespeare.fileids()]\n",
    "shakespeare_model = train_word2vec(shakespeare_corpus)\n",
    "\n",
    "# Approx. number of words in Shakespeare Corpus\n",
    "\n",
    "num_words = sum(len(file.split(' ')) for file in shakespeare_corpus)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "caf8ee5e-d78b-44e1-b14e-7d7cc3bf6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on a temporary corpus, using roughly the same amount of data as with the Shakespeare corpus to make it a fair comparison\n",
    "\n",
    "contemporary_corpus = [' '.join(reuters.words(fileid)) for fileid in reuters.fileids()]\n",
    "\n",
    "contemporary_words = 0\n",
    "for i in range(0, len(contemporary_corpus)):\n",
    "    contemporary_words += len(contemporary_corpus[i].split(' '))\n",
    "    if contemporary_words > num_words:\n",
    "        break\n",
    "\n",
    "contemporary_corpus = contemporary_corpus[:i]\n",
    "contemporary_model = train_word2vec(contemporary_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7dc9160e-5781-43d5-858e-a756ae8d86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "X_train_shakespeare = [get_document_vector(doc, shakespeare_model) for doc in X_train]\n",
    "X_test_shakespeare = [get_document_vector(doc, shakespeare_model) for doc in X_test]\n",
    "\n",
    "X_train_contemporary = [get_document_vector(doc, contemporary_model) for doc in X_train]\n",
    "X_test_contemporary = [get_document_vector(doc, contemporary_model) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9683fd76-6db1-407a-a3b1-69ce2ccc5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    positives += (y_test[i] == 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "dd554909-d53a-4cc1-bebd-6887c7d91468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53.33% of the test examples are positive\n",
      "Accuracy using Shakespeare embeddings: 0.5866666666666667\n",
      "Accuracy using Contemporary embeddings: 0.62\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate classifiers\n",
    "def evaluate(X_train, X_test, y_train, y_test):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    return accuracy_score(y_test, predictions)\n",
    "\n",
    "accuracy_shakespeare = evaluate(X_train_shakespeare, X_test_shakespeare, y_train, y_test)\n",
    "accuracy_contemporary = evaluate(X_train_contemporary, X_test_contemporary, y_train, y_test)\n",
    "\n",
    "print(f\" {(positives / len(y_test)) * 100:.2f}% of the test examples are positive\")\n",
    "print(f\"Accuracy using Shakespeare embeddings: {accuracy_shakespeare}\")\n",
    "print(f\"Accuracy using Contemporary embeddings: {accuracy_contemporary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a526f-6f8d-4f0a-a94d-0bca23fbd8bb",
   "metadata": {},
   "source": [
    "We can see that out of all the movie reviews from the test set, 53% were positive. Of our two models trained:\n",
    "\n",
    "The one trained on Shakespeare achieves an accuracy of 58.7%\n",
    "The one trained on Reuters data achieves an accuracy of 62%\n",
    "\n",
    "Both are better than a simple coin toss (50-50 odds), so there is some signal there. However, these models could improve a lot. Some of the reasons I suspect they are not doing very well are:\n",
    "\n",
    "1. Shakespeare's data uses an outdated language that is not commonly used today, so it does not resemble the language used in movie reviews. Although Reuters uses a more contemporary language, it is still very niche and heavily focused on news, which might not be exactly the style of language used for a movie review. Therefore, word2vec is creating embeddings on a writing style that does not resemble much the writing style on which it was trained on, which is probably causing it not to create very meaningful embeddings.\n",
    "2. A logistic regression model might not be the best one to use for this classification task. A neural network or an ensemble model like Random Forest would probably do better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
