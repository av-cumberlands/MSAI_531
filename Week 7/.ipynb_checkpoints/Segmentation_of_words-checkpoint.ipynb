{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa5f77-8593-42f4-b41b-ba05aec0daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8886e9ef-5a6a-4770-9240-62dc2d63fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unigram model (word frequencies) using a public dataset\n",
    "\n",
    "def load_unigram_model():\n",
    "    \"\"\"\n",
    "    Load word frequencies from Peter Norvig's compilation of Google Books Ngram data\n",
    "    Source: https://norvig.com/ngrams/count_1w.txt\n",
    "    Format: word\\tcount\n",
    "    \"\"\"\n",
    "    url = \"https://norvig.com/ngrams/count_1w.txt\"\n",
    "    \n",
    "    # Download the word frequency data\n",
    "    response = urllib.request.urlopen(url)\n",
    "    words_data = response.read().decode('utf-8').splitlines()\n",
    "    \n",
    "    # Parse into word: frequency dictionary\n",
    "    word_frequencies = {}\n",
    "    total_words = 0\n",
    "    \n",
    "    for line in words_data:\n",
    "        word, count = line.strip().split('\\t')\n",
    "        count = int(count)\n",
    "        word_frequencies[word] = count\n",
    "        total_words += count\n",
    "    \n",
    "    # Convert to log probabilities\n",
    "    word_prob = {}\n",
    "    for word, count in word_frequencies.items():\n",
    "        word_prob[word] = math.log(count / total_words)\n",
    "    \n",
    "    # Add a small probability for unknown words\n",
    "    word_prob['<UNK>'] = math.log(1 / (total_words))\n",
    "    \n",
    "    return word_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d7884097-0d42-4edf-8406-bc3baa96e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Word Segmentation Algorithm\n",
    "\n",
    "def segment_words(text, word_prob):\n",
    "    # Initialize variables\n",
    "    n = len(text)\n",
    "    best_segment = [0] * (n + 1)  # Stores the index where the best segment ends\n",
    "    best_score = [float('inf')] * (n + 1)  # Stores the best score for each position\n",
    "    best_score[0] = 0  # The start of the text has zero cost\n",
    "\n",
    "    # Dynamic Programming Loop\n",
    "    for i in range(1, n + 1):\n",
    "        # Check substrings ending at position i, limiting the substring length to 20\n",
    "        for j in range(max(0, i - 20), i):\n",
    "            word = text[j:i]  # Extract substring\n",
    "            if word in word_prob:  # If the substring is a valid word\n",
    "                score = best_score[j] - word_prob[word]  # Calculate score (lower is better)\n",
    "                if score < best_score[i]:  # Update if this segmentation is better\n",
    "                    best_score[i] = score\n",
    "                    best_segment[i] = j\n",
    "\n",
    "    # Backtrack to reconstruct the segmented words\n",
    "    words = []\n",
    "    i = n\n",
    "    while i > 0:\n",
    "        j = best_segment[i]\n",
    "        words.append(text[j:i])\n",
    "        i = j\n",
    "\n",
    "    return words[::-1]  # Reverse the list since we constructed it backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "326d12e8-45ef-4df9-aa5e-c959e381a745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: ['the', 'longest', 'list', 'of', 'the', 'longest', 'stuff', 'at', 'the', 'longest', 'domainname', 'at', 'long', 'last']\n"
     ]
    }
   ],
   "source": [
    "word_model = load_unigram_model()\n",
    "\n",
    "# Input URL\n",
    "\n",
    "url = \"thelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com\"\n",
    "    \n",
    "# Perform word segmentation\n",
    "\n",
    "result = segment_words(text, word_model)\n",
    "print(f\"Result: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
